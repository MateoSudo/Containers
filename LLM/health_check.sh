#!/bin/bash

# Health Check Script for LLM Docker Compose Stack

echo "üîç Health Check for LLM Stack"
echo "=============================="

# Check if Docker is running
if ! docker info > /dev/null 2>&1; then
    echo "‚ùå Docker is not running"
    exit 1
fi

# Check if docker-compose.yml exists
if [ ! -f "docker-compose.yml" ]; then
    echo "‚ùå docker-compose.yml not found"
    exit 1
fi

# Check service status
echo ""
echo "üìä Service Status:"
docker-compose ps

echo ""
echo "üîó Testing service connectivity..."

# Test Open WebUI
echo "Testing Open WebUI (port 3000)..."
if curl -s http://localhost:3000 > /dev/null; then
    echo "‚úÖ Open WebUI is accessible"
else
    echo "‚ùå Open WebUI is not accessible"
fi

# Test Ollama
echo "Testing Ollama API (port 11434)..."
if curl -s http://localhost:11434/api/tags > /dev/null; then
    echo "‚úÖ Ollama API is accessible"
else
    echo "‚ùå Ollama API is not accessible"
fi

# Test Jupyter
echo "Testing Jupyter (port 8888)..."
if curl -s http://localhost:8888 > /dev/null; then
    echo "‚úÖ Jupyter is accessible"
else
    echo "‚ùå Jupyter is not accessible"
fi

# Test Text Generation WebUI
echo "Testing Text Generation WebUI (port 7860)..."
if curl -s http://localhost:7860 > /dev/null; then
    echo "‚úÖ Text Generation WebUI is accessible"
else
    echo "‚ùå Text Generation WebUI is not accessible"
fi

# Check GPU support
echo ""
echo "üéÆ GPU Support:"
if docker run --rm --gpus all nvidia/cuda:11.0-base nvidia-smi > /dev/null 2>&1; then
    echo "‚úÖ GPU support detected"
    docker run --rm --gpus all nvidia/cuda:11.0-base nvidia-smi --query-gpu=name,memory.total --format=csv,noheader,nounits
else
    echo "‚ö†Ô∏è  GPU support not detected"
fi

# Check disk space
echo ""
echo "üíæ Disk Space:"
df -h . | tail -1

# Check memory usage
echo ""
echo "üß† Memory Usage:"
free -h

# Check running containers
echo ""
echo "üê≥ Running Containers:"
docker ps --format "table {{.Names}}\t{{.Status}}\t{{.Ports}}"

echo ""
echo "üìã Summary:"
echo "‚Ä¢ Open WebUI: http://localhost:3000"
echo "‚Ä¢ Jupyter: http://localhost:8888"
echo "‚Ä¢ Ollama API: http://localhost:11434"
echo "‚Ä¢ Text Generation WebUI: http://localhost:7860"
echo ""
echo "üìö Useful commands:"
echo "‚Ä¢ View logs: docker-compose logs -f [service_name]"
echo "‚Ä¢ Stop services: docker-compose down"
echo "‚Ä¢ Restart services: docker-compose restart"
echo "‚Ä¢ Pull models: docker-compose exec ollama ollama pull llama2:7b" 